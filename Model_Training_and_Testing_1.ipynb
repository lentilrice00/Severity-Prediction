{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Development**\n",
        "For the study, ANN model would be used. The loss function used during training is sparse categorical crossentropy. The model is trained on Google Colab T4 free GPU for 10 epochs with model checkpoint and early stopping patience of 3 with dataset of batch_size 32.\n",
        "\n",
        "**Limitation in this project**: Here, due to lack of resources, model tuning will not be performed.\n"
      ],
      "metadata": {
        "id": "KFyQIopo8HHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s8N6zexEEW5",
        "outputId": "f08130f6-aeb7-4cb5-9655-402afb242558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Crash severity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooMRXA28EKV9",
        "outputId": "63e90312-c6e0-4a51-a74d-a63baf13fb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1PPTdMShCmN_ebwQDrXakKkI4ggw-4Ayf/Crash severity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "y_train = pd.read_csv('y_train.csv')\n",
        "chunksize = 10000\n",
        "chunks = []\n",
        "for chunk in pd.read_csv('X_train.csv', chunksize=chunksize, dtype=np.float16):\n",
        "  chunks.append(chunk)\n",
        "X_train = pd.concat(chunks, ignore_index=True)"
      ],
      "metadata": {
        "id": "eT9kyS908QUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free memory by deleting variables from system memory\n",
        "for var in list(globals().keys()):\n",
        "  if var not in ['X_train','y_train'] and not var.startswith('__'):\n",
        "    del globals()[var]\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "print(globals().keys())"
      ],
      "metadata": {
        "id": "b5UGcnjeFg-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a76d112-2b91-45bd-b8d7-465758dad3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '__', '___', 'y_train', 'X_train', 'var', 'gc'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_dataset(X, y, batch_size, is_training=True):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "  if is_training:\n",
        "    dataset = dataset.shuffle(buffer_size=len(X))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n"
      ],
      "metadata": {
        "id": "HLSp21VLxRpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_size = int(0.7 * len(X_train))\n",
        "train_dataset = create_dataset(X_train[:train_size], y_train[:train_size], batch_size).apply(tf.data.experimental.prefetch_to_device('/gpu:0'))\n",
        "val_dataset = create_dataset(X_train[train_size:], y_train[train_size:], batch_size, is_training=False).apply(tf.data.experimental.prefetch_to_device('/gpu:0'))"
      ],
      "metadata": {
        "id": "aai_D8X_vtLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free memory by deleting variables from system memory\n",
        "for var in list(globals().keys()):\n",
        "  if var not in ['train_dataset','val_dataset'] and not var.startswith('__'):\n",
        "    del globals()[var]\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "print(globals().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVdgtkwyx0nk",
        "outputId": "13e9e61d-bb53-4a1b-b639-ff1c59138b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '__', '___', 'train_dataset', 'val_dataset', 'var', 'gc'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJbt5uVf8DsF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "for features, labels in train_dataset.take(1):\n",
        "  input_shape = features.shape[1:]\n",
        "\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "x = tf.keras.layers.Dense(64,activation='relu')(inputs)\n",
        "x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(4,activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs,outputs)\n",
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change initial_epoch if system crashes"
      ],
      "metadata": {
        "id": "C8JtPIMWIM1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('best_model.keras')\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    initial_epoch = 8,\n",
        "    epochs = 10,\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath='best_model.keras',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            mode='min',\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience = 3\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4U2L8I7xx8s",
        "outputId": "6b2ad452-b108-4f8b-b7c9-a66537f432d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10\n",
            " 135247/Unknown \u001b[1m259s\u001b[0m 2ms/step - accuracy: 0.8014 - loss: 0.4773"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9: val_loss improved from inf to 0.47717, saving model to best_model.keras\n",
            "\u001b[1m135247/135247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 2ms/step - accuracy: 0.8014 - loss: 0.4773 - val_accuracy: 0.8019 - val_loss: 0.4772\n",
            "Epoch 10/10\n",
            "\u001b[1m135238/135247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.4767\n",
            "Epoch 10: val_loss did not improve from 0.47717\n",
            "\u001b[1m135247/135247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.4767 - val_accuracy: 0.8002 - val_loss: 0.4777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Interpretation\n",
        "\n",
        "The best model for ANN is loaded for evaluation. After evaluation it was realized that inclusion of other road characteristics data such as shoulder width, injury severity, different road characteristics would have enhanced the quality of this study. Additionally, a computer with better computational resources would also have helped in the study to delve deeper through tuning. Since ANN is a \"black-box\" model, it was incorporated with Shapley Additive exPlanations (SHAP) to explain the feature important for prediction of severity.\n"
      ],
      "metadata": {
        "id": "T0pRjvZe8bx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(X, y, batch_size, is_training=True):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "  if is_training:\n",
        "    dataset = dataset.shuffle(buffer_size=len(X))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "reUev5MMjA27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "X_test = pd.read_csv('X_test.csv',dtype=np.float16)\n",
        "y_test = pd.read_csv('y_test.csv')"
      ],
      "metadata": {
        "id": "2Vrf4tPDjKC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = create_dataset(X_test, y_test, batch_size=32, is_training=False).apply(tf.data.experimental.prefetch_to_device('/gpu:0'))"
      ],
      "metadata": {
        "id": "JnlJ6g-qjl-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('best_model.keras')\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJR6O2Y3j18G",
        "outputId": "60dec498-3014-4b7b-8ebe-0d0529207273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m48303/48303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.4777\n",
            "Test Loss: 0.4774448275566101\n",
            "Test Accuracy: 0.8020319938659668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        }
      ]
    }
  ]
}